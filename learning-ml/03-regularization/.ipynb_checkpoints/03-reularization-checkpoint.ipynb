{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daae0676-3fed-4946-bc1d-aedc8a97ff41",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a954817-902f-4436-a9e4-46a6badeb34e",
   "metadata": {},
   "source": [
    "#### In general we can think of regularization as a way to reduce overfitting and varience,\n",
    " #### 1. This required some additonal bias\n",
    " #### 2. Requires a search for optimal penalty hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87aa028f-9491-4461-b2e4-6a92a016edeb",
   "metadata": {},
   "source": [
    "#### There are 3 main types of regularization.\n",
    "##### 1. L1 reg. or LASSO regression\n",
    "##### 2. L2 reg. or Ridge regression\n",
    "##### 3. Combinging L1 and L2 or Elastic net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3110fd-d1d0-48cd-a551-d84d27feaab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca180d2-22e0-4ca1-91a0-e1830a2e21d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a0b3ef3-210f-48bf-9ef3-b94dea61880c",
   "metadata": {},
   "source": [
    "#### Good example for ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc12429f-c47d-4475-b41d-e7a21070c20c",
   "metadata": {},
   "source": [
    "Alright, here‚Äôs **regularization** explained super simply:\n",
    "\n",
    "---\n",
    "\n",
    "### Imagine you're fitting a line to some data points (that‚Äôs a linear model).\n",
    "\n",
    "You want the line to predict well **not just for the training data**, but also for **new, unseen data**.\n",
    "\n",
    "Now sometimes, the model becomes too smart for its own good‚Ä¶\n",
    "\n",
    "It **bends itself too much** to match the training data perfectly ‚Äî that‚Äôs called **overfitting**.\n",
    "\n",
    "---\n",
    "\n",
    "### Regularization is like telling your model:\n",
    "\n",
    "> ‚ÄúYo, don‚Äôt go crazy trying to fit everything perfectly. Keep it simple and chill.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "### How does it do that?\n",
    "\n",
    "It **penalizes** the model when it uses **very large weights** (the numbers that control the slope of the line).\n",
    "Why? Because large weights usually mean the model is trying too hard to fit the training data.\n",
    "\n",
    "Regularization **adds a small punishment** to the model‚Äôs error (loss), based on how large the weights are.\n",
    "\n",
    "---\n",
    "\n",
    "### There are two common flavors:\n",
    "\n",
    "#### 1. **L1 Regularization (Lasso)**\n",
    "\n",
    "* Adds penalty = sum of absolute values of weights\n",
    "* Can make some weights go all the way to zero (i.e., removes unnecessary features)\n",
    "\n",
    "#### 2. **L2 Regularization (Ridge)**\n",
    "\n",
    "* Adds penalty = sum of squares of weights\n",
    "* Makes weights smaller, but rarely exactly zero\n",
    "\n",
    "---\n",
    "\n",
    "### Analogy time üß†:\n",
    "\n",
    "Imagine you‚Äôre trying to draw a straight line through a cloud of dots on a whiteboard.\n",
    "\n",
    "* **Without regularization:** You let your hand be as shaky or twisty as it wants ‚Äî might fit all dots perfectly, but looks messy.\n",
    "* **With regularization:** It‚Äôs like putting a little resistance band on your hand ‚Äî keeps your movement smooth and simple.\n",
    "\n",
    "---\n",
    "\n",
    "So TL;DR:\n",
    "**Regularization = a way to keep the model from overfitting by discouraging it from relying too much on any one input.**\n",
    "It's like adding some ‚Äúcommon sense‚Äù to your model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28f5c71-7e00-43a7-bb34-1074e8156381",
   "metadata": {},
   "source": [
    "Oh hell yes ‚Äî this is where the fun starts. Let‚Äôs break this down in the simplest, most visual way possible ‚Äî with a bit of **geometry and intuition**.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† First, understand what optimization is doing:\n",
    "\n",
    "In linear regression (without regularization), you're just minimizing the **loss** (say, Mean Squared Error).\n",
    "With regularization, you‚Äôre minimizing:\n",
    "\n",
    "```\n",
    "Loss = MSE + Regularization Term\n",
    "```\n",
    "\n",
    "So it becomes a **tug-of-war** between:\n",
    "\n",
    "* Fitting the data well (**MSE**)\n",
    "* Keeping the model simple (**regularization term**)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Let's now focus on why L1 can zero out weights but L2 usually doesn‚Äôt.\n",
    "\n",
    "---\n",
    "\n",
    "## üü® Think in 2D: Imagine a graph with just **2 coefficients: w1 and w2**\n",
    "\n",
    "### 1. **L1 Regularization (Lasso)**:\n",
    "\n",
    "It adds this penalty:\n",
    "\n",
    "```\n",
    "|w1| + |w2|\n",
    "```\n",
    "\n",
    "And this forms a **diamond-shaped constraint region** like this:\n",
    "\n",
    "```\n",
    "     |\n",
    "  \\  |  /\n",
    "   \\ | /       <- diamond (sharp corners)\n",
    "---- + ----\n",
    "   / | \\\n",
    "  /  |  \\\n",
    "     |\n",
    "```\n",
    "\n",
    "These sharp corners **lie exactly on the axes** ‚Äî where **w1 or w2 = 0**.\n",
    "\n",
    "So when the optimization tries to find the minimum error **inside this diamond**, it's super likely to land on one of those corners ‚Äî which means **setting w1 or w2 exactly to 0**.\n",
    "\n",
    "üëâ That‚Äôs why **L1 regularization leads to sparsity** ‚Äî it kills off unnecessary weights.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **L2 Regularization (Ridge)**:\n",
    "\n",
    "It adds this penalty:\n",
    "\n",
    "```\n",
    "w1¬≤ + w2¬≤\n",
    "```\n",
    "\n",
    "And that forms a **circle (or ellipse) constraint region**:\n",
    "\n",
    "```\n",
    "     ‚Ä¢\n",
    "  ‚Ä¢     ‚Ä¢\n",
    " ‚Ä¢       ‚Ä¢\n",
    "  ‚Ä¢     ‚Ä¢\n",
    "     ‚Ä¢\n",
    "```\n",
    "\n",
    "No sharp corners here, just smooth curves.\n",
    "\n",
    "When you minimize the loss inside this region, you‚Äôre **less likely to hit exact zeros**.\n",
    "You‚Äôll get **small weights**, but not **exactly zero**.\n",
    "\n",
    "üëâ So **L2 shrinks all weights**, but doesn‚Äôt usually kill them.\n",
    "\n",
    "---\n",
    "\n",
    "### üî• Bottom Line:\n",
    "\n",
    "| Regularization | Penalty Shape | What It Does      | Zeroes Out Coeffs? |\n",
    "| -------------- | ------------- | ----------------- | ------------------ |\n",
    "| L1 (Lasso)     | Diamond       | Feature selection | ‚úÖ Yes              |\n",
    "| L2 (Ridge)     | Circle        | Shrinkage         | ‚ùå Rarely           |\n",
    "\n",
    "---\n",
    "\n",
    "### Bonus Analogy üéØ\n",
    "\n",
    "* **L1 is like firing people** from a company: \"You‚Äôre not useful? You're out!\"\n",
    "* **L2 is like reducing salaries**: \"You‚Äôre kinda useful? I‚Äôll just pay you less.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a976b7-85d2-4bb4-8e4c-91ffec72d00c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e7bb24-234f-47a6-b3b4-e868432fb1c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28fac999-0dfc-4c2c-82ed-48578a35632b",
   "metadata": {},
   "source": [
    "#### Feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7157ab18-5a4c-421f-bc21-466498cecd62",
   "metadata": {},
   "source": [
    "Hell yes. Let‚Äôs do **feature scaling** the **street-smart way** ‚Äî no books, just intuition.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† What is Feature Scaling?\n",
    "\n",
    "Imagine you‚Äôre training a model, and your input has:\n",
    "\n",
    "* `age`: 20 to 80\n",
    "* `income`: 20,000 to 2,00,000\n",
    "* `height`: 1.5 to 2 meters\n",
    "\n",
    "Now you throw these into a model like a salad.\n",
    "\n",
    "**Problem:**\n",
    "The model sees `income` is way bigger than `height`, so it starts thinking **income is more important**, even when it‚Äôs not.\n",
    "\n",
    "> Feature scaling is just making sure **everything is on the same playing field**.\n",
    "\n",
    "So `age`, `income`, `height` ‚Äî all become like:\n",
    "`0.1`, `0.5`, `0.9` ‚Äî same scale, fair fight.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Why do we care?\n",
    "\n",
    "Some models (like Linear Regression, KNN, SVM, Gradient Descent-based models) **get confused** if features have wildly different scales.\n",
    "They think big numbers = big importance.\n",
    "\n",
    "So we scale them to stop that nonsense.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Types of Feature Scaling (with brain-dead analogies):\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Min-Max Scaling (Normalization)**\n",
    "\n",
    "```python\n",
    "x_scaled = (x - min) / (max - min)\n",
    "```\n",
    "\n",
    "**What it does:**\n",
    "Squishes values between **0 and 1**.\n",
    "\n",
    "**Analogy:**\n",
    "Imagine shrinking every player to fit between the heights of the shortest and tallest ‚Äî now everyone's between 0 (shortest) and 1 (tallest).\n",
    "\n",
    "**Used when:**\n",
    "You know your data limits and want things in a specific range (e.g., neural networks love this).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Standardization (Z-Score Scaling)**\n",
    "\n",
    "```python\n",
    "x_scaled = (x - mean) / std\n",
    "```\n",
    "\n",
    "**What it does:**\n",
    "Transforms data to have:\n",
    "\n",
    "* Mean = 0\n",
    "* Standard Deviation = 1\n",
    "\n",
    "**Analogy:**\n",
    "Imagine you center everyone around \"average\", and measure how weird or far off they are from it.\n",
    "\n",
    "**Used when:**\n",
    "You don‚Äôt know the exact bounds of the data or want to keep outliers around. SVM, Logistic Regression, PCA ‚Äî all love this.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **MaxAbs Scaling**\n",
    "\n",
    "```python\n",
    "x_scaled = x / max(abs(x))\n",
    "```\n",
    "\n",
    "**What it does:**\n",
    "Keeps **signs (positive/negative)** intact, scales values to -1 to +1.\n",
    "\n",
    "**Analogy:**\n",
    "Like min-max, but doesn‚Äôt mess with 0. Good for **sparse data** (lots of 0s).\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Robust Scaling**\n",
    "\n",
    "```python\n",
    "x_scaled = (x - median) / IQR\n",
    "```\n",
    "\n",
    "Where IQR = interquartile range (75th percentile - 25th percentile)\n",
    "\n",
    "**What it does:**\n",
    "Ignores outliers, focuses on the middle chunk of the data.\n",
    "\n",
    "**Analogy:**\n",
    "You‚Äôre trying to compare people‚Äôs height, but you ignore the giants and dwarfs. You care about the **normal crowd**.\n",
    "\n",
    "**Used when:**\n",
    "Your data has outliers that you don't want to distort the scaling.\n",
    "\n",
    "---\n",
    "\n",
    "## üí° TL;DR Quick Chart\n",
    "\n",
    "| Type         | Range     | Sensitive to Outliers? | When to Use                             |\n",
    "| ------------ | --------- | ---------------------- | --------------------------------------- |\n",
    "| Min-Max      | 0 to 1    | ‚úÖ Yes                  | Neural nets, bounded features           |\n",
    "| Standard (Z) | \\~-3 to 3 | ‚úÖ Yes                  | Most ML algorithms (SVM, LR, PCA, etc.) |\n",
    "| MaxAbs       | -1 to 1   | ‚úÖ Yes                  | Sparse data                             |\n",
    "| Robust       | Depends   | ‚ùå No                   | Data with outliers                      |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ed241b-3e3a-416e-8426-f0864c510111",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e292821a-bbc8-44ef-ab12-274c95aea403",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data setup for regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "441e8f01-48b6-47b7-ba28-7eb8f0816ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e5a9aea-5397-467a-bc97-3f8438d02611",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../pierian-data-excercises/08-Linear-Regression-Models/Advertising.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62609f74-50f1-48ed-9bf6-201d434109a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('sales', axis=1)\n",
    "y = df['sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fe33a0d-30e3-4bab-990e-9713249fc12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efb32953-4fb9-4f6c-9624-f0d64e6e9911",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_converter = PolynomialFeatures(degree=3, include_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8864c845-5bea-4044-9116-1da87a6eb7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_features = poly_converter.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "624a2fd0-4777-4c3f-80af-e07f515690ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "877d04e3-364c-4477-a263-91091db383fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 19)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77360a5b-7a23-4fec-be23-f8ae171740ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77cc8b1f-7914-44ee-aa83-a6f473427a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(poly_features, y, test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8368e8aa-252e-44d5-af01-6d37ef1da8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will scale the features now\n",
    "# We dont do it before splitting so that the test data doesn't leaks into training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efecf145-fa99-4418-ba80-6d2f95001a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5dbc68cf-1320-44af-841f-c3605e6e1857",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a63de3d3-e495-485d-be94-7c81aed37966",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaler.fit_transform(X_train) # Scaling the X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b02062d4-067d-48af-b6e6-605cc5c9f008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.49300171, -0.33994238,  1.61586707,  0.28407363, -0.02568776,\n",
       "        1.49677566, -0.59023161,  0.41659155,  1.6137853 ,  0.08057172,\n",
       "       -0.05392229,  1.01524393, -0.36986163,  0.52457967,  1.48737034,\n",
       "       -0.66096022, -0.16360242,  0.54694754,  1.37075536])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0] # See the values are much smaller now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78ff1cd7-bd32-4361-898f-5424b37b1fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9a27789-27da-42cd-b486-1e438a682c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.70089624,  1.77066512,  0.73398854, -0.813449  ,  0.160092  ,\n",
       "       -0.10365708,  2.2314938 ,  1.47174292,  0.36412244, -0.76998264,\n",
       "       -0.40201033, -0.49954437,  0.5041369 ,  0.38482416, -0.04050793,\n",
       "        2.56792918,  1.8877582 ,  0.77106615,  0.07159576])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5bd2d7-9e9d-4464-bd54-fb813873edaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadd48c2-250a-434f-a367-a50c27ab1591",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ec6b6a0-cd04-47b4-89aa-d3378c04f51a",
   "metadata": {},
   "source": [
    "#### L2 Regularization impl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4afb2ba-ff53-47ff-a951-070e9ea8a295",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
