{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daae0676-3fed-4946-bc1d-aedc8a97ff41",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a954817-902f-4436-a9e4-46a6badeb34e",
   "metadata": {},
   "source": [
    "#### In general we can think of regularization as a way to reduce overfitting and varience,\n",
    " #### 1. This required some additonal bias\n",
    " #### 2. Requires a search for optimal penalty hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87aa028f-9491-4461-b2e4-6a92a016edeb",
   "metadata": {},
   "source": [
    "#### There are 3 main types of regularization.\n",
    "##### 1. L1 reg. or LASSO regression\n",
    "##### 2. L2 reg. or Ridge regression\n",
    "##### 3. Combinging L1 and L2 or Elastic net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3110fd-d1d0-48cd-a551-d84d27feaab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca180d2-22e0-4ca1-91a0-e1830a2e21d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a0b3ef3-210f-48bf-9ef3-b94dea61880c",
   "metadata": {},
   "source": [
    "#### Good example for ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc12429f-c47d-4475-b41d-e7a21070c20c",
   "metadata": {},
   "source": [
    "Alright, here‚Äôs **regularization** explained super simply:\n",
    "\n",
    "---\n",
    "\n",
    "### Imagine you're fitting a line to some data points (that‚Äôs a linear model).\n",
    "\n",
    "You want the line to predict well **not just for the training data**, but also for **new, unseen data**.\n",
    "\n",
    "Now sometimes, the model becomes too smart for its own good‚Ä¶\n",
    "\n",
    "It **bends itself too much** to match the training data perfectly ‚Äî that‚Äôs called **overfitting**.\n",
    "\n",
    "---\n",
    "\n",
    "### Regularization is like telling your model:\n",
    "\n",
    "> ‚ÄúYo, don‚Äôt go crazy trying to fit everything perfectly. Keep it simple and chill.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "### How does it do that?\n",
    "\n",
    "It **penalizes** the model when it uses **very large weights** (the numbers that control the slope of the line).\n",
    "Why? Because large weights usually mean the model is trying too hard to fit the training data.\n",
    "\n",
    "Regularization **adds a small punishment** to the model‚Äôs error (loss), based on how large the weights are.\n",
    "\n",
    "---\n",
    "\n",
    "### There are two common flavors:\n",
    "\n",
    "#### 1. **L1 Regularization (Lasso)**\n",
    "\n",
    "* Adds penalty = sum of absolute values of weights\n",
    "* Can make some weights go all the way to zero (i.e., removes unnecessary features)\n",
    "\n",
    "#### 2. **L2 Regularization (Ridge)**\n",
    "\n",
    "* Adds penalty = sum of squares of weights\n",
    "* Makes weights smaller, but rarely exactly zero\n",
    "\n",
    "---\n",
    "\n",
    "### Analogy time üß†:\n",
    "\n",
    "Imagine you‚Äôre trying to draw a straight line through a cloud of dots on a whiteboard.\n",
    "\n",
    "* **Without regularization:** You let your hand be as shaky or twisty as it wants ‚Äî might fit all dots perfectly, but looks messy.\n",
    "* **With regularization:** It‚Äôs like putting a little resistance band on your hand ‚Äî keeps your movement smooth and simple.\n",
    "\n",
    "---\n",
    "\n",
    "So TL;DR:\n",
    "**Regularization = a way to keep the model from overfitting by discouraging it from relying too much on any one input.**\n",
    "It's like adding some ‚Äúcommon sense‚Äù to your model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28f5c71-7e00-43a7-bb34-1074e8156381",
   "metadata": {},
   "source": [
    "Oh hell yes ‚Äî this is where the fun starts. Let‚Äôs break this down in the simplest, most visual way possible ‚Äî with a bit of **geometry and intuition**.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† First, understand what optimization is doing:\n",
    "\n",
    "In linear regression (without regularization), you're just minimizing the **loss** (say, Mean Squared Error).\n",
    "With regularization, you‚Äôre minimizing:\n",
    "\n",
    "```\n",
    "Loss = MSE + Regularization Term\n",
    "```\n",
    "\n",
    "So it becomes a **tug-of-war** between:\n",
    "\n",
    "* Fitting the data well (**MSE**)\n",
    "* Keeping the model simple (**regularization term**)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Let's now focus on why L1 can zero out weights but L2 usually doesn‚Äôt.\n",
    "\n",
    "---\n",
    "\n",
    "## üü® Think in 2D: Imagine a graph with just **2 coefficients: w1 and w2**\n",
    "\n",
    "### 1. **L1 Regularization (Lasso)**:\n",
    "\n",
    "It adds this penalty:\n",
    "\n",
    "```\n",
    "|w1| + |w2|\n",
    "```\n",
    "\n",
    "And this forms a **diamond-shaped constraint region** like this:\n",
    "\n",
    "```\n",
    "     |\n",
    "  \\  |  /\n",
    "   \\ | /       <- diamond (sharp corners)\n",
    "---- + ----\n",
    "   / | \\\n",
    "  /  |  \\\n",
    "     |\n",
    "```\n",
    "\n",
    "These sharp corners **lie exactly on the axes** ‚Äî where **w1 or w2 = 0**.\n",
    "\n",
    "So when the optimization tries to find the minimum error **inside this diamond**, it's super likely to land on one of those corners ‚Äî which means **setting w1 or w2 exactly to 0**.\n",
    "\n",
    "üëâ That‚Äôs why **L1 regularization leads to sparsity** ‚Äî it kills off unnecessary weights.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **L2 Regularization (Ridge)**:\n",
    "\n",
    "It adds this penalty:\n",
    "\n",
    "```\n",
    "w1¬≤ + w2¬≤\n",
    "```\n",
    "\n",
    "And that forms a **circle (or ellipse) constraint region**:\n",
    "\n",
    "```\n",
    "     ‚Ä¢\n",
    "  ‚Ä¢     ‚Ä¢\n",
    " ‚Ä¢       ‚Ä¢\n",
    "  ‚Ä¢     ‚Ä¢\n",
    "     ‚Ä¢\n",
    "```\n",
    "\n",
    "No sharp corners here, just smooth curves.\n",
    "\n",
    "When you minimize the loss inside this region, you‚Äôre **less likely to hit exact zeros**.\n",
    "You‚Äôll get **small weights**, but not **exactly zero**.\n",
    "\n",
    "üëâ So **L2 shrinks all weights**, but doesn‚Äôt usually kill them.\n",
    "\n",
    "---\n",
    "\n",
    "### üî• Bottom Line:\n",
    "\n",
    "| Regularization | Penalty Shape | What It Does      | Zeroes Out Coeffs? |\n",
    "| -------------- | ------------- | ----------------- | ------------------ |\n",
    "| L1 (Lasso)     | Diamond       | Feature selection | ‚úÖ Yes              |\n",
    "| L2 (Ridge)     | Circle        | Shrinkage         | ‚ùå Rarely           |\n",
    "\n",
    "---\n",
    "\n",
    "### Bonus Analogy üéØ\n",
    "\n",
    "* **L1 is like firing people** from a company: \"You‚Äôre not useful? You're out!\"\n",
    "* **L2 is like reducing salaries**: \"You‚Äôre kinda useful? I‚Äôll just pay you less.\"\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you want a quick plot or code demo for this ‚Äî I can spin one up.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a976b7-85d2-4bb4-8e4c-91ffec72d00c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e7bb24-234f-47a6-b3b4-e868432fb1c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28fac999-0dfc-4c2c-82ed-48578a35632b",
   "metadata": {},
   "source": [
    "#### Feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7157ab18-5a4c-421f-bc21-466498cecd62",
   "metadata": {},
   "source": [
    "Hell yes. Let‚Äôs do **feature scaling** the **street-smart way** ‚Äî no books, just intuition.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† What is Feature Scaling?\n",
    "\n",
    "Imagine you‚Äôre training a model, and your input has:\n",
    "\n",
    "* `age`: 20 to 80\n",
    "* `income`: 20,000 to 2,00,000\n",
    "* `height`: 1.5 to 2 meters\n",
    "\n",
    "Now you throw these into a model like a salad.\n",
    "\n",
    "**Problem:**\n",
    "The model sees `income` is way bigger than `height`, so it starts thinking **income is more important**, even when it‚Äôs not.\n",
    "\n",
    "> Feature scaling is just making sure **everything is on the same playing field**.\n",
    "\n",
    "So `age`, `income`, `height` ‚Äî all become like:\n",
    "`0.1`, `0.5`, `0.9` ‚Äî same scale, fair fight.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Why do we care?\n",
    "\n",
    "Some models (like Linear Regression, KNN, SVM, Gradient Descent-based models) **get confused** if features have wildly different scales.\n",
    "They think big numbers = big importance.\n",
    "\n",
    "So we scale them to stop that nonsense.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Types of Feature Scaling (with brain-dead analogies):\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Min-Max Scaling (Normalization)**\n",
    "\n",
    "```python\n",
    "x_scaled = (x - min) / (max - min)\n",
    "```\n",
    "\n",
    "**What it does:**\n",
    "Squishes values between **0 and 1**.\n",
    "\n",
    "**Analogy:**\n",
    "Imagine shrinking every player to fit between the heights of the shortest and tallest ‚Äî now everyone's between 0 (shortest) and 1 (tallest).\n",
    "\n",
    "**Used when:**\n",
    "You know your data limits and want things in a specific range (e.g., neural networks love this).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Standardization (Z-Score Scaling)**\n",
    "\n",
    "```python\n",
    "x_scaled = (x - mean) / std\n",
    "```\n",
    "\n",
    "**What it does:**\n",
    "Transforms data to have:\n",
    "\n",
    "* Mean = 0\n",
    "* Standard Deviation = 1\n",
    "\n",
    "**Analogy:**\n",
    "Imagine you center everyone around \"average\", and measure how weird or far off they are from it.\n",
    "\n",
    "**Used when:**\n",
    "You don‚Äôt know the exact bounds of the data or want to keep outliers around. SVM, Logistic Regression, PCA ‚Äî all love this.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **MaxAbs Scaling**\n",
    "\n",
    "```python\n",
    "x_scaled = x / max(abs(x))\n",
    "```\n",
    "\n",
    "**What it does:**\n",
    "Keeps **signs (positive/negative)** intact, scales values to -1 to +1.\n",
    "\n",
    "**Analogy:**\n",
    "Like min-max, but doesn‚Äôt mess with 0. Good for **sparse data** (lots of 0s).\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Robust Scaling**\n",
    "\n",
    "```python\n",
    "x_scaled = (x - median) / IQR\n",
    "```\n",
    "\n",
    "Where IQR = interquartile range (75th percentile - 25th percentile)\n",
    "\n",
    "**What it does:**\n",
    "Ignores outliers, focuses on the middle chunk of the data.\n",
    "\n",
    "**Analogy:**\n",
    "You‚Äôre trying to compare people‚Äôs height, but you ignore the giants and dwarfs. You care about the **normal crowd**.\n",
    "\n",
    "**Used when:**\n",
    "Your data has outliers that you don't want to distort the scaling.\n",
    "\n",
    "---\n",
    "\n",
    "## üí° TL;DR Quick Chart\n",
    "\n",
    "| Type         | Range     | Sensitive to Outliers? | When to Use                             |\n",
    "| ------------ | --------- | ---------------------- | --------------------------------------- |\n",
    "| Min-Max      | 0 to 1    | ‚úÖ Yes                  | Neural nets, bounded features           |\n",
    "| Standard (Z) | \\~-3 to 3 | ‚úÖ Yes                  | Most ML algorithms (SVM, LR, PCA, etc.) |\n",
    "| MaxAbs       | -1 to 1   | ‚úÖ Yes                  | Sparse data                             |\n",
    "| Robust       | Depends   | ‚ùå No                   | Data with outliers                      |\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you want a hands-on demo on how each scaling looks with matplotlib + sklearn ‚Äî super easy to grok.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ed241b-3e3a-416e-8426-f0864c510111",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
